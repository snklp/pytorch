{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "thdGs-ZSDAzQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter ######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNC5_5TxVM7F"
   },
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ijRNAia-DFyk"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(self, ip_channels=1, op_features=10):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(ip_channels, 8, 3, (1,1), (1,1) )  # ip_channnels, op_channels, kernel_size, stride, padding\n",
    "    self.conv2 = nn.Conv2d(8, 16, 3, (1,1), (1,1))\n",
    "    self.maxpool = nn.MaxPool2d((2,2), (2,2))   # kernel_size, stride\n",
    "    self.fc1 = nn.Linear(16*7*7, op_features)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.maxpool(x)\n",
    "    x = self.conv2(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.maxpool(x)\n",
    "    x = x.view(x.shape[0], -1)\n",
    "    x = self.fc1(x)\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WgPTPJAaDJfM"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-hHu9ODH81V"
   },
   "source": [
    "**Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Z68kk1CDF3M4"
   },
   "outputs": [],
   "source": [
    "ip_channels = 1\n",
    "op_features = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1QUVmYXHR2W"
   },
   "source": [
    "**Loading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "kLUCtdd2GMy9"
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='Data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='Data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-Bjp7VXdxVl",
    "outputId": "a7cb8510-cb34-400a-a83d-2650341c9d9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "2\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 28, 28])\n",
      "<class 'int'>\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataset:\n",
    "  print(type(i))\n",
    "  print(len(i))\n",
    "  print(type(i[0]))\n",
    "  print(i[0].shape)\n",
    "  print(type(i[1]))\n",
    "  print(i[1])\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "stami3avdxLN",
    "outputId": "c7c492be-51fe-4ec8-ffd8-6f94391bcd28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "2\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([8, 6, 9, 9, 3, 6, 0, 6, 6, 1, 4, 3, 1, 1, 3, 9, 4, 0, 8, 9, 3, 3, 7, 3,\n",
      "        7, 1, 9, 1, 2, 2, 6, 0, 7, 8, 7, 9, 0, 6, 7, 3, 4, 3, 6, 9, 8, 7, 0, 7,\n",
      "        2, 3, 8, 1, 2, 2, 6, 2, 7, 5, 1, 5, 2, 1, 4, 7])\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "  print(type(i))\n",
    "  print(len(i))\n",
    "  print(type(i[0]))\n",
    "  print(i[0].shape)\n",
    "  print(type(i[1]))\n",
    "  print(i[1])\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [1,64,128, 1024]\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBHOr21pOGrt"
   },
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VppXOVHH3L5",
    "outputId": "7b0cffbc-0fb5-4c88-b9c2-fe0fa59bc5b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Mean Loss:0.045598321139744456\n",
      "Epoch:1, Mean Loss:0.04102607664584156\n",
      "Epoch:2, Mean Loss:0.03667895719153322\n",
      "Epoch:3, Mean Loss:0.03308674022577691\n",
      "Epoch:4, Mean Loss:0.03019424989157523\n"
     ]
    }
   ],
   "source": [
    "for batch_size in batch_sizes:\n",
    "    for learning_rate in learning_rates:\n",
    "        step = 0\n",
    "        model.train() ########################################\n",
    "        model = CNN().to(device)\n",
    "        \n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0)\n",
    "        writer = SummaryWriter(f'runs/MNIST/MiniBatchSize {batch_size} LR {learning_rate}')\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            losses = []\n",
    "            accuracies = []\n",
    "            for batch_index, data in enumerate(train_loader):\n",
    "                X, y = data\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                y_ = model(X)\n",
    "                loss = loss_function(y_, y)\n",
    "                losses.append(loss)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                _, predictions = scores.max(dim=1)\n",
    "                correct = (predictions == y).sum()\n",
    "                acc = float(correct)/float(X.shape[0])\n",
    "                \n",
    "                writer.add_scalar('TrainingLoss', loss, global_step=step)\n",
    "                writer.add_scalar('TrainingAccuracy', acc, global_step=step)\n",
    "                accuracies.append(acc)\n",
    "                \n",
    "            print(f'Epoch:{epoch}, MeanLoss:{sum(losses)/len(losses)}')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! tensorboard --logdir=runs #for tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0yDLIpQVWqf"
   },
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "F95-Ff6CH4nv"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "  for data in test_loader:\n",
    "    X, y = data\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    y_ = model(X)\n",
    "    \n",
    "    _, predictions = y_.max(dim=1)\n",
    "    correct += (predictions== y).sum()\n",
    "    total += predictions.shape[0]\n",
    "\n",
    "    #for idx, op_class in enumerate(y_):\n",
    "      #if torch.argmax(op_class) == y[idx]:correct += 1\n",
    "      #total += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpDvrphUVZ7O"
   },
   "source": [
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WfoznDIyH9y9",
    "outputId": "543096c6-0af4-4366-eb70-e926040a9664"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9715)"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZokpzjoH-0P"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TensorBoard.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
